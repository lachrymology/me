---
title: JS Class-likes
filemdate: 2012.10.31
---

I got an interesting question from Juan Ignacio Dopazo on Twitter about which is optimised more: the so-called closure object pattern (objects as closures), or the prototypal inheritance pattern. And Juan used some jsFiddle.net website to put up what he was talking about, so I could see the code examples, and I think we're going to have links to some Gists for them on GitHub.

The closure pattern is where you write a constructor (lets call it "ClosurePattern"), and typically, it assigns to "this.someMethod" an anonymous function or a named function expression. And then that results in a new method if you call the expression "new ClosurePattern()". You get back an object, and it has a property named "someMethod". In fact it's a direct property; it's an "own" property, in ECMA's terms. And people think this is more efficient. I think Doug Crockford advocates the objects-as-closures pattern in "Javascript: the good parts".

The other way to do it is known as the prototypal pattern. And there are lots of variations on these, so this isn't like, the only two ways to do it. The prototypal just makes a constructor (let's call it "function PrototypalPattern", and it's just got an empty body: it doesn't set this.anything. It does, however, then have a "prototype" property, which is assigned (usually) using a global assignment statement: `PrototypalPattern.prototype = { someMethod: <function expression> }`. And then similarly you call "new PrototypalPattern()" and you get back a fresh object, but that object does not have a direct (or "own") property called "someMethod": instead, it delegates to its prototype (which is "PrototypalPattern.prototype"), and that's where the method "someMethod" lives.

The interesting thing about this is that people are concerned that going up the prototype chain is expensive. It turns out that in all the modern JavaScript engines, this has been optimised: in Firefox 3 and beyond, we optimised this using various styles of polymorphic inline caching, or property lookup caching. So it doesn't matter how far you look up the prototype chain: if a particular piece of code is accessing "foo.someMethod", calling it, let's say, and you run that code a lot, you're going to fill a cache that lets you teleport directly to "someMethod". There may be a little check to make sure that someone hasn't shadowed that prototype property with an "own" property that has a different value, but that can be a very fast test.

The closure pattern, the one where you have the method as a function expression inside the constructor, can actually be more expensive. If you think about it, each time you call that constructor (let's say you call it a million times), you have to evaluate the method function expression. You have to evaluate the assignment "this.someMethod = function() { /*etc*/ }". In JavaScript, every time you evaluate a function expression, you get a new object. You can tell it's a new object because of "==="; you can also mutate it: it's mutable. So you can set a certain property on it that would not show up on any other instances -- "someMethod". You'd say "new ClosurePattern()" and assign that to x, and also set y to "new ClosurePattern()". Now you've got two instances. x.someMethod !== y.someMethod, and "x.someMethod.foo = 42" would not cause 42 to appear on y.someMethod: they'd be different objects. That hurts! If you do a million constructions, you've got not just a million instances of the ClosurePattern object, you've got a million "someMethod"s inside it.

Now this can be optimised. In Firefox (I don't know if other engines do this: I haven't seen it), we do something that I saw done in Chez Scheme, Kent Dybvig's Scheme implementation, which he called "display closures". We try to optimise so that if you have a function inside another function, you don't have to keep the outer function's activation object around. That's one optimisation, and that helps. If the method is not using any names from the constructor, if it's only using global names like "console" to call "console.log", or it's only using its arguments, that is, it's a very pure function, then it can be more efficient.

The other thing that might help here is even more aggressive optimisation, that I don't think other engines do, and that's called "joined methods" or "joined function objects". This was something that ECMAScript 3rd Edition tried to allow explicitly, as a choice implementations could make. It meant that if you called ClosurePattern a million times, and you looked at x.someMethod and y.someMethod and z.someMethod and so on, they'd all be the same object in some implementations -- and in others, they wouldn't. That's a bad idea because they are mutable: it's observable through that setting of "foo" on the method itself that you have one method shared amongst all these instances or you do not. And (of course) you can also tell with "==" or "===". So it didn't seem like a good idea, and it was removed from ES5. But it can be done as an optimisation, so long as you hide the joining, so that if someone does mutate, or someone does ask "are these things the same?", you've magically arranged for different functions -- fresh function objects to be created, as if they were created ahead of time, when the constructor ran. Thus you can optimise so that when you call that ClosurePattern a million times, you don't have a million "someMethod"s, as if in a naive implementation of the ECMA spec.

So the closure pattern does not win over the prototypal pattern in efficiency, and this is somewhat counter-intuitive. People think looking up the prototype chain is more expensive than a nested function, but a nested function can be more expensive. But neither of these costs is necessarily dominant: you shouldn't have to worry about these normally, and you shouldn't prematurely optimise or write your code as if you're tuning for one particular implementation of JavaScript. I remember, years ago, we used to get advice from Intel as they were changing their micro-architecture: they would say, "Oh, let's not use shift instructions any more: we've only got one barrel shifter, and it's not that fast, and it's a scarce resource on the chip. So stop using shift operators in your C code." And we were, like, "No! We're going to keep using shift, or we're going to use multiply and divide, and the compiler will turn that into shift, but it's up to the compiler to choose the best way to do this."

I think, to Doug's credit, he's advocating a style that gives better encapsulation: with the closure pattern, you can have private variables. And indeed, you can not use "this" at all: this is one of the variations that I didn't mention, where the closure pattern can have inner functions that are private; it can also have ones that it publishes. But it does them by putting them into a new object literal that "return"s, and it doesn't touch "this" at all. And then, when you call the ClosurePattern constructor, you don't even use the "new" operator. Or, if you do use the "new" operator, the return value trumps the nominal new object that would have been created due to the "new" operator. You get back that object initialiser inside the ClosurePattern constructor, and it's always a fresh object. But you still have this issue I mentioned: you still have the issue that every call theoretically could create a fresh set of methods. If there are four methods, for instance, and each of them is written as an expression assigned to "this.someMethod" or initialised as the first value of a property in an object initialiser that's returned by the constructor: either way, you're going to end up with a million instances with four million total methods hanging off them in a naive implementation of ECMAScript. So, it's something to watch out for. Again, if you're only making a few instances, or it's not a matter of performance (you haven't measured performance being hit by this), then you can use the closure pattern in good health, and it can have good encapsulation wins for you.

But to get into the bean counting, and look at it from a performance point of view, and to look at how current engines generally don't optimise closures that much, and do not join function objects together when they can, closure patterns can be more expensive than prototypal. I should note: you might think that when you're writing nested functions, the compiler's only compiling it once: it's sees the outer function, it sees the inner function -- there are two functions, should be cheap, right? Again, it's this evaluation rule: that every time you evaluate a function expression, you get a fresh object: it's got a separate identity, it's mutable. That potentially makes for high cost here. The optimisations to join the function objects so that they're one-to-one with their source forms, with what the compiler sees and processes, is a little bit more work, I would say, having done it in SpiderMonkey, than the caching-up-the-prototype-chain lookups. That is better known (and older!) technology. The escape analysis, the sort of "read" and "write" barriers required for joined function operators optimisations  are heavier-weight and harder, and they tend to not always work. That means you've to un-join: you've got to clone the function sometimes, and you're paying the full price then.

Again, you always want to measure your code; you don't want to prematurely optimise it, so I hesitate to be seen to be endorsing the prototypal pattern. I don't take a religious position here. I think closure patterns are good for encapsulation; prototypal patterns are old and quite convenient for JavaScripters who are just hacking, and don't need to use very strong encapsulation. People have different opinions on this: they always think you need strong encapsulation when you scale up your code, so you may as well use it from the start; other people think, "well, maybe my code doesn't scale up, and I'm in a hurry; I'll do it later". Again, there are trade-offs: it's up to you, as developers, to choose the right ones for you.

